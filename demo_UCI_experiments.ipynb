{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo UCI experiments \n",
    "\n",
    "This notebook is intended to present some of our results on the UCI dataset.\n",
    "The source code for our project can be found in the /code/ folder.\n",
    "In this notebook, we train a not_MIWAE model on the UCI Breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### First type of missing data\n",
    "In this first part, we tackle a new type of missing data, that was not presented in the original paper. We introduce missing values by removing the most extreme values of the dataset in half of the features. We remove the values located in the first and last quartile.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:39:28.202432Z",
     "start_time": "2024-12-11T13:39:28.127148Z"
    }
   },
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import datetime\n",
    "from code.common.data_imputation import compute_imputation_rmse_not_miwae\n",
    "from code.code_UCI_experiments.not_miwae import notMIWAE, get_notMIWAE\n",
    "from code.code_UCI_experiments.train import train_notMIWAE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from code.code_UCI_experiments.introduce_missing_data import introduce_missing_mean_values, introduce_missing_extreme_values"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:39:29.592890Z",
     "start_time": "2024-12-11T13:39:29.341549Z"
    }
   },
   "source": [
    "# Loading data\n",
    "data = np.array(pd.read_csv('datasets/cancer-dataset/Cancer_Data.csv', low_memory=False, sep=','))\n",
    "X_data = data[:, 2:-2]  # Features"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:39:30.526072Z",
     "start_time": "2024-12-11T13:39:30.415849Z"
    }
   },
   "source": [
    "# Split data into train and validation sets\n",
    "Xtrain, Xval = train_test_split(X_data, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Compute mean and standard deviation from the training set only\n",
    "mean_train  = np.mean(Xtrain.astype(np.float64), axis=0)\n",
    "std_train   = np.std(Xtrain.astype(np.float64), axis=0)\n",
    "\n",
    "# Standardize the training set using its mean and std\n",
    "Xtrain      = (Xtrain - mean_train) / std_train\n",
    "total_samples_x_train = Xtrain.shape[0]\n",
    "# Standardize the validation set using the training set's mean and std\n",
    "Xval = (Xval - mean_train) / std_train\n",
    "\n",
    "# Introduce missing data to features\n",
    "Xnan_train, Xz_train    = introduce_missing_extreme_values(Xtrain)\n",
    "Xnan_val, Xz_val        = introduce_missing_extreme_values(Xval)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing missing data via removing extreme values\n",
      "Introducing missing data via removing extreme values\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:24:59.164191Z",
     "start_time": "2024-12-11T13:24:59.107784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create missing data masks (1 if present, 0 if missing)\n",
    "Strain  = torch.tensor(~np.isnan(Xnan_train), dtype=torch.float32)\n",
    "Sval    = torch.tensor(~np.isnan(Xnan_val), dtype=torch.float32)\n",
    "\n",
    "Xtrain  = Xtrain.astype(np.float32)\n",
    "Xval    = Xval.astype(np.float32)\n",
    "\n",
    "# Convert features and target to PyTorch tensors\n",
    "Xnan_train  = torch.tensor(Xnan_train, dtype=torch.float32)\n",
    "Xnan_val    = torch.tensor(Xnan_val, dtype=torch.float32)\n",
    "Xtrain      = torch.tensor(Xtrain, dtype= torch.float32)\n",
    "Xval        = torch.tensor(Xval, dtype= torch.float32)\n",
    "\n",
    "# Replace missing values (NaN) with zeros for training\n",
    "Xnan_train[torch.isnan(Xnan_train)]     = 0\n",
    "Xnan_val[torch.isnan(Xnan_val)]         = 0\n",
    "\n",
    "# Prepare TensorDatasets and DataLoaders for features (X), mask (S), and target (y)\n",
    "train_dataset   = TensorDataset(Xnan_train, Strain, Xtrain) # Features, mask, true features\n",
    "val_dataset     = TensorDataset(Xnan_val, Sval, Xval)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:14:58.840650Z",
     "start_time": "2024-12-11T13:14:58.822492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "calib_config = [\n",
    "                {'model': 'not_miwae', 'dataset_name':  'cancer', 'lr': 1e-4, 'epochs' : 100, 'pct_start': 0.2, 'final_div_factor': 1e4, 'batch_size': 16, 'n_hidden': 128, 'n_latent': 28, 'missing_process':'linear', 'weight_decay': 0, 'betas': (0.9, 0.999), 'random_seed': 0, 'out_dist': 'gauss'},\n",
    "\n",
    "                ][-1]"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:14:59.253506Z",
     "start_time": "2024-12-11T13:14:59.192285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoaders\n",
    "train_loader    = DataLoader(train_dataset, batch_size=calib_config['batch_size'], shuffle=True)\n",
    "val_loader      = DataLoader(val_dataset, batch_size=calib_config['batch_size'], shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:14:59.658472Z",
     "start_time": "2024-12-11T13:14:59.589428Z"
    }
   },
   "cell_type": "code",
   "source": "model = notMIWAE(n_input_features=Xtrain.shape[1], n_hidden=calib_config['n_hidden'], n_latent = calib_config['n_latent'], missing_process = calib_config['missing_process'], out_dist=calib_config['out_dist'])\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the BernouilliDecoderMiss: 870\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:15:49.100105Z",
     "start_time": "2024-12-11T13:15:00.122766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "date = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "print(f\"Training timestamp : {date}\")\n",
    "print(f\"Number of parameters in the model: {sum (p.numel() if p.requires_grad else 0 for p in model.parameters()) }\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=calib_config['lr'], weight_decay=calib_config['weight_decay'], betas=calib_config['betas'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr = calib_config['lr'],\n",
    "                                                epochs = calib_config['epochs'],\n",
    "                                                steps_per_epoch= len(train_loader),\n",
    "                                                pct_start= calib_config['pct_start'],\n",
    "                                                )\n",
    "print(f\"calib_config:{calib_config}\")\n",
    "logging.info(\"Starting training\")\n",
    "if calib_config['model'] == 'not_miwae':\n",
    "    train_notMIWAE(model, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, scheduler = scheduler, num_epochs = calib_config['epochs'], total_samples_x_train=total_samples_x_train, device = device, date=date)\n",
    "    torch.save(model.state_dict(), f\"temp/not_miwae_{date}_last_epoch.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training timestamp : 2024_12_11_14_15_00\n",
      "Number of parameters in the model: 56152\n",
      "calib_config:{'model': 'not_miwae', 'dataset_name': 'cancer', 'lr': 0.0001, 'epochs': 100, 'pct_start': 0.2, 'final_div_factor': 10000.0, 'batch_size': 16, 'n_hidden': 128, 'n_latent': 28, 'missing_process': 'linear', 'weight_decay': 0, 'betas': (0.9, 0.999), 'random_seed': 0, 'out_dist': 'gauss'}\n",
      "Epoch    1, Train Loss:  44.1113 , Train rmse:  1.7057 , Val Loss:  46.0794 , Val RMSE:  1.5257  last value of lr: 0.0000\n",
      "Epoch    2, Train Loss:  44.1120 , Train rmse:  1.6841 , Val Loss:  45.9932 , Val RMSE:  1.5830  last value of lr: 0.0000\n",
      "Epoch    3, Train Loss:  44.3436 , Train rmse:  1.6801 , Val Loss:  45.6282 , Val RMSE:  1.5767  last value of lr: 0.0000\n",
      "Epoch    4, Train Loss:  44.1224 , Train rmse:  1.6198 , Val Loss:  45.8035 , Val RMSE:  1.5873  last value of lr: 0.0000\n",
      "Epoch    5, Train Loss:  44.1230 , Train rmse:  1.6653 , Val Loss:  45.5128 , Val RMSE:  1.5763  last value of lr: 0.0000\n",
      "Epoch    6, Train Loss:  44.2901 , Train rmse:  1.6707 , Val Loss:  45.7428 , Val RMSE:  1.5850  last value of lr: 0.0000\n",
      "Epoch    7, Train Loss:  43.9139 , Train rmse:  1.6327 , Val Loss:  45.4501 , Val RMSE:  1.5751  last value of lr: 0.0000\n",
      "Epoch    8, Train Loss:  43.7395 , Train rmse:  1.6528 , Val Loss:  45.2636 , Val RMSE:  1.5262  last value of lr: 0.0000\n",
      "Epoch    9, Train Loss:  43.7423 , Train rmse:  1.6594 , Val Loss:  45.1646 , Val RMSE:  1.5289  last value of lr: 0.0000\n",
      "Epoch   10, Train Loss:  43.5433 , Train rmse:  1.6109 , Val Loss:  44.9407 , Val RMSE:  1.5193  last value of lr: 0.0001\n",
      "Epoch   11, Train Loss:  43.3912 , Train rmse:  1.6510 , Val Loss:  44.7787 , Val RMSE:  1.4956  last value of lr: 0.0001\n",
      "Epoch   12, Train Loss:  43.0267 , Train rmse:  1.6161 , Val Loss:  44.5593 , Val RMSE:  1.5639  last value of lr: 0.0001\n",
      "Epoch   13, Train Loss:  42.9080 , Train rmse:  1.6658 , Val Loss:  44.1362 , Val RMSE:  1.5673  last value of lr: 0.0001\n",
      "Epoch   14, Train Loss:  42.3811 , Train rmse:  1.6532 , Val Loss:  43.7757 , Val RMSE:  1.4673  last value of lr: 0.0001\n",
      "Epoch   15, Train Loss:  42.0119 , Train rmse:  1.5986 , Val Loss:  42.6747 , Val RMSE:  1.5164  last value of lr: 0.0001\n",
      "Epoch   16, Train Loss:  41.3509 , Train rmse:  1.6043 , Val Loss:  42.3116 , Val RMSE:  1.5659  last value of lr: 0.0001\n",
      "Epoch   17, Train Loss:  40.7824 , Train rmse:  1.6371 , Val Loss:  41.0112 , Val RMSE:  1.5327  last value of lr: 0.0001\n",
      "Epoch   18, Train Loss:  39.6123 , Train rmse:  1.5986 , Val Loss:  40.3851 , Val RMSE:  1.5065  last value of lr: 0.0001\n",
      "Epoch   19, Train Loss:  38.9538 , Train rmse:  1.6421 , Val Loss:  39.2114 , Val RMSE:  1.5866  last value of lr: 0.0001\n",
      "Epoch   20, Train Loss:  38.5511 , Train rmse:  1.6368 , Val Loss:  39.2449 , Val RMSE:  1.6213  last value of lr: 0.0001\n",
      "Epoch   21, Train Loss:  38.0034 , Train rmse:  1.6383 , Val Loss:  39.3024 , Val RMSE:  1.6418  last value of lr: 0.0001\n",
      "Epoch   22, Train Loss:  37.2885 , Train rmse:  1.6180 , Val Loss:  38.3702 , Val RMSE:  1.5392  last value of lr: 0.0001\n",
      "Epoch   23, Train Loss:  37.0788 , Train rmse:  1.6368 , Val Loss:  38.1317 , Val RMSE:  1.5547  last value of lr: 0.0001\n",
      "Epoch   24, Train Loss:  36.7316 , Train rmse:  1.5985 , Val Loss:  37.4145 , Val RMSE:  1.6293  last value of lr: 0.0001\n",
      "Epoch   25, Train Loss:  37.1543 , Train rmse:  1.6227 , Val Loss:  37.5635 , Val RMSE:  1.5394  last value of lr: 0.0001\n",
      "Epoch   26, Train Loss:  36.7765 , Train rmse:  1.6430 , Val Loss:  37.5503 , Val RMSE:  1.5801  last value of lr: 0.0001\n",
      "Epoch   27, Train Loss:  37.4358 , Train rmse:  1.6174 , Val Loss:  36.9634 , Val RMSE:  1.5129  last value of lr: 0.0001\n",
      "Epoch   28, Train Loss:  35.7214 , Train rmse:  1.6256 , Val Loss:  36.7259 , Val RMSE:  1.5619  last value of lr: 0.0001\n",
      "Epoch   29, Train Loss:  35.9975 , Train rmse:  1.6387 , Val Loss:  36.7123 , Val RMSE:  1.6437  last value of lr: 0.0001\n",
      "Epoch   30, Train Loss:  35.6832 , Train rmse:  1.6353 , Val Loss:  36.7047 , Val RMSE:  1.4897  last value of lr: 0.0001\n",
      "Epoch   31, Train Loss:  35.1558 , Train rmse:  1.6528 , Val Loss:  36.2818 , Val RMSE:  1.5927  last value of lr: 0.0001\n",
      "Epoch   32, Train Loss:  35.5175 , Train rmse:  1.6521 , Val Loss:  36.7311 , Val RMSE:  1.5524  last value of lr: 0.0001\n",
      "Epoch   33, Train Loss:  36.8929 , Train rmse:  1.6264 , Val Loss:  36.4013 , Val RMSE:  1.5120  last value of lr: 0.0001\n",
      "Epoch   34, Train Loss:  36.1101 , Train rmse:  1.6381 , Val Loss:  36.6165 , Val RMSE:  1.5732  last value of lr: 0.0001\n",
      "Epoch   35, Train Loss:  36.2754 , Train rmse:  1.6368 , Val Loss:  36.3820 , Val RMSE:  1.5766  last value of lr: 0.0001\n",
      "Epoch   36, Train Loss:  35.9223 , Train rmse:  1.6406 , Val Loss:  36.4374 , Val RMSE:  1.6112  last value of lr: 0.0001\n",
      "Epoch   37, Train Loss:  36.5581 , Train rmse:  1.6775 , Val Loss:  35.9274 , Val RMSE:  1.5072  last value of lr: 0.0001\n",
      "Epoch   38, Train Loss:  36.4222 , Train rmse:  1.6591 , Val Loss:  36.0566 , Val RMSE:  1.5684  last value of lr: 0.0001\n",
      "Epoch   39, Train Loss:  35.0849 , Train rmse:  1.6525 , Val Loss:  36.0269 , Val RMSE:  1.6290  last value of lr: 0.0001\n",
      "Epoch   40, Train Loss:  36.4273 , Train rmse:  1.6703 , Val Loss:  36.1650 , Val RMSE:  1.6111  last value of lr: 0.0001\n",
      "Epoch   41, Train Loss:  34.7994 , Train rmse:  1.6587 , Val Loss:  35.9907 , Val RMSE:  1.6326  last value of lr: 0.0001\n",
      "Epoch   42, Train Loss:  36.2749 , Train rmse:  1.6449 , Val Loss:  35.7530 , Val RMSE:  1.5524  last value of lr: 0.0001\n",
      "Epoch   43, Train Loss:  35.1938 , Train rmse:  1.6778 , Val Loss:  35.9377 , Val RMSE:  1.5769  last value of lr: 0.0001\n",
      "Epoch   44, Train Loss:  35.3316 , Train rmse:  1.6491 , Val Loss:  35.8453 , Val RMSE:  1.5235  last value of lr: 0.0001\n",
      "Epoch   45, Train Loss:  34.6948 , Train rmse:  1.6563 , Val Loss:  35.5470 , Val RMSE:  1.5784  last value of lr: 0.0001\n",
      "Epoch   46, Train Loss:  35.5730 , Train rmse:  1.6699 , Val Loss:  35.8319 , Val RMSE:  1.6059  last value of lr: 0.0001\n",
      "Epoch   47, Train Loss:  35.6555 , Train rmse:  1.6490 , Val Loss:  35.5046 , Val RMSE:  1.6058  last value of lr: 0.0001\n",
      "Epoch   48, Train Loss:  35.7303 , Train rmse:  1.6391 , Val Loss:  35.6032 , Val RMSE:  1.5917  last value of lr: 0.0001\n",
      "Epoch   49, Train Loss:  35.0962 , Train rmse:  1.6076 , Val Loss:  35.2121 , Val RMSE:  1.6600  last value of lr: 0.0001\n",
      "Epoch   50, Train Loss:  34.8893 , Train rmse:  1.6563 , Val Loss:  35.6652 , Val RMSE:  1.6568  last value of lr: 0.0001\n",
      "Epoch   51, Train Loss:  34.4933 , Train rmse:  1.6815 , Val Loss:  35.7553 , Val RMSE:  1.6087  last value of lr: 0.0001\n",
      "Epoch   52, Train Loss:  35.3058 , Train rmse:  1.6607 , Val Loss:  35.5060 , Val RMSE:  1.5948  last value of lr: 0.0001\n",
      "Epoch   53, Train Loss:  35.0491 , Train rmse:  1.6158 , Val Loss:  35.4650 , Val RMSE:  1.5869  last value of lr: 0.0001\n",
      "Epoch   54, Train Loss:  34.6567 , Train rmse:  1.6701 , Val Loss:  35.3449 , Val RMSE:  1.5945  last value of lr: 0.0001\n",
      "Epoch   55, Train Loss:  34.1819 , Train rmse:  1.6524 , Val Loss:  35.3005 , Val RMSE:  1.5687  last value of lr: 0.0001\n",
      "Epoch   56, Train Loss:  35.4556 , Train rmse:  1.6636 , Val Loss:  35.4062 , Val RMSE:  1.5764  last value of lr: 0.0001\n",
      "Epoch   57, Train Loss:  34.7324 , Train rmse:  1.5947 , Val Loss:  35.1169 , Val RMSE:  1.5894  last value of lr: 0.0001\n",
      "Epoch   58, Train Loss:  34.3537 , Train rmse:  1.6725 , Val Loss:  35.3772 , Val RMSE:  1.5382  last value of lr: 0.0001\n",
      "Epoch   59, Train Loss:  35.3248 , Train rmse:  1.6252 , Val Loss:  35.3909 , Val RMSE:  1.5776  last value of lr: 0.0001\n",
      "Epoch   60, Train Loss:  35.0115 , Train rmse:  1.6490 , Val Loss:  35.1856 , Val RMSE:  1.6022  last value of lr: 0.0000\n",
      "Epoch   61, Train Loss:  35.8300 , Train rmse:  1.6510 , Val Loss:  35.4403 , Val RMSE:  1.5963  last value of lr: 0.0000\n",
      "Epoch   62, Train Loss:  35.3302 , Train rmse:  1.6482 , Val Loss:  35.2634 , Val RMSE:  1.5982  last value of lr: 0.0000\n",
      "Epoch   63, Train Loss:  35.3636 , Train rmse:  1.6574 , Val Loss:  35.3456 , Val RMSE:  1.6139  last value of lr: 0.0000\n",
      "Epoch   64, Train Loss:  35.0982 , Train rmse:  1.6703 , Val Loss:  35.1910 , Val RMSE:  1.5417  last value of lr: 0.0000\n",
      "Epoch   65, Train Loss:  34.7428 , Train rmse:  1.6512 , Val Loss:  35.0550 , Val RMSE:  1.5640  last value of lr: 0.0000\n",
      "Epoch   66, Train Loss:  35.4793 , Train rmse:  1.6346 , Val Loss:  35.1152 , Val RMSE:  1.6199  last value of lr: 0.0000\n",
      "Epoch   67, Train Loss:  34.0648 , Train rmse:  1.6115 , Val Loss:  35.0790 , Val RMSE:  1.5838  last value of lr: 0.0000\n",
      "Epoch   68, Train Loss:  34.2052 , Train rmse:  1.6584 , Val Loss:  34.8639 , Val RMSE:  1.5657  last value of lr: 0.0000\n",
      "Epoch   69, Train Loss:  34.3477 , Train rmse:  1.6534 , Val Loss:  34.9318 , Val RMSE:  1.5949  last value of lr: 0.0000\n",
      "Epoch   70, Train Loss:  34.8347 , Train rmse:  1.6442 , Val Loss:  34.9390 , Val RMSE:  1.5258  last value of lr: 0.0000\n",
      "Epoch   71, Train Loss:  34.8071 , Train rmse:  1.6260 , Val Loss:  35.0779 , Val RMSE:  1.6207  last value of lr: 0.0000\n",
      "Epoch   72, Train Loss:  34.8165 , Train rmse:  1.6714 , Val Loss:  34.9111 , Val RMSE:  1.7822  last value of lr: 0.0000\n",
      "Epoch   73, Train Loss:  34.3448 , Train rmse:  1.6755 , Val Loss:  35.0175 , Val RMSE:  1.5540  last value of lr: 0.0000\n",
      "Epoch   74, Train Loss:  34.9792 , Train rmse:  1.6554 , Val Loss:  34.9834 , Val RMSE:  1.7050  last value of lr: 0.0000\n",
      "Epoch   75, Train Loss:  34.1967 , Train rmse:  1.6133 , Val Loss:  34.7860 , Val RMSE:  1.7006  last value of lr: 0.0000\n",
      "Epoch   76, Train Loss:  34.2240 , Train rmse:  1.6966 , Val Loss:  34.7070 , Val RMSE:  1.6755  last value of lr: 0.0000\n",
      "Epoch   77, Train Loss:  34.8611 , Train rmse:  1.7254 , Val Loss:  35.0674 , Val RMSE:  1.5932  last value of lr: 0.0000\n",
      "Epoch   78, Train Loss:  34.7884 , Train rmse:  1.6302 , Val Loss:  34.8102 , Val RMSE:  1.5927  last value of lr: 0.0000\n",
      "Epoch   79, Train Loss:  34.6217 , Train rmse:  1.6479 , Val Loss:  34.7999 , Val RMSE:  1.5831  last value of lr: 0.0000\n",
      "Epoch   80, Train Loss:  35.3933 , Train rmse:  1.6480 , Val Loss:  34.9444 , Val RMSE:  1.6555  last value of lr: 0.0000\n",
      "Epoch   81, Train Loss:  33.9347 , Train rmse:  1.6543 , Val Loss:  34.7537 , Val RMSE:  1.5833  last value of lr: 0.0000\n",
      "Epoch   82, Train Loss:  35.0843 , Train rmse:  1.6529 , Val Loss:  34.8187 , Val RMSE:  1.5639  last value of lr: 0.0000\n",
      "Epoch   83, Train Loss:  34.8612 , Train rmse:  1.6408 , Val Loss:  34.7105 , Val RMSE:  1.5752  last value of lr: 0.0000\n",
      "Epoch   84, Train Loss:  34.0649 , Train rmse:  1.6382 , Val Loss:  34.6589 , Val RMSE:  1.6544  last value of lr: 0.0000\n",
      "Epoch   85, Train Loss:  34.4344 , Train rmse:  1.7045 , Val Loss:  34.9074 , Val RMSE:  1.5275  last value of lr: 0.0000\n",
      "Epoch   86, Train Loss:  34.1833 , Train rmse:  1.6101 , Val Loss:  34.9316 , Val RMSE:  1.5438  last value of lr: 0.0000\n",
      "Epoch   87, Train Loss:  34.3706 , Train rmse:  1.6106 , Val Loss:  34.7905 , Val RMSE:  1.6423  last value of lr: 0.0000\n",
      "Epoch   88, Train Loss:  34.7621 , Train rmse:  1.6656 , Val Loss:  34.8384 , Val RMSE:  1.6087  last value of lr: 0.0000\n",
      "Epoch   89, Train Loss:  33.9067 , Train rmse:  1.6442 , Val Loss:  34.8553 , Val RMSE:  1.5756  last value of lr: 0.0000\n",
      "Epoch   90, Train Loss:  34.1170 , Train rmse:  1.6774 , Val Loss:  34.7290 , Val RMSE:  1.5810  last value of lr: 0.0000\n",
      "Epoch   91, Train Loss:  33.5991 , Train rmse:  1.7035 , Val Loss:  34.7819 , Val RMSE:  1.5379  last value of lr: 0.0000\n",
      "Epoch   92, Train Loss:  34.1765 , Train rmse:  1.6567 , Val Loss:  34.6056 , Val RMSE:  1.5716  last value of lr: 0.0000\n",
      "Epoch   93, Train Loss:  34.0726 , Train rmse:  1.6723 , Val Loss:  34.9203 , Val RMSE:  1.5828  last value of lr: 0.0000\n",
      "Epoch   94, Train Loss:  34.8113 , Train rmse:  1.6264 , Val Loss:  34.7253 , Val RMSE:  1.5725  last value of lr: 0.0000\n",
      "Epoch   95, Train Loss:  34.5290 , Train rmse:  1.6541 , Val Loss:  34.8026 , Val RMSE:  1.5545  last value of lr: 0.0000\n",
      "Epoch   96, Train Loss:  34.2437 , Train rmse:  1.6191 , Val Loss:  34.6776 , Val RMSE:  1.5422  last value of lr: 0.0000\n",
      "Epoch   97, Train Loss:  34.1622 , Train rmse:  1.5953 , Val Loss:  34.7943 , Val RMSE:  1.5174  last value of lr: 0.0000\n",
      "Epoch   98, Train Loss:  33.3787 , Train rmse:  1.6595 , Val Loss:  35.1230 , Val RMSE:  1.6294  last value of lr: 0.0000\n",
      "Epoch   99, Train Loss:  34.0872 , Train rmse:  1.6307 , Val Loss:  34.8690 , Val RMSE:  1.5778  last value of lr: 0.0000\n",
      "Epoch  100, Train Loss:  33.5720 , Train rmse:  1.6322 , Val Loss:  34.8198 , Val RMSE:  1.6111  last value of lr: 0.0000\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:15:49.194771Z",
     "start_time": "2024-12-11T13:15:49.113497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the best model\n",
    "model.load_state_dict(torch.load(f\"temp/not_miwae_{date}_best_val_loss.pt\", weights_only=True))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:15:49.579858Z",
     "start_time": "2024-12-11T13:15:49.515163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get best validation accuracy\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "val_rmse = 0\n",
    "with torch.no_grad():\n",
    "    for x, s, xtrue in val_loader:\n",
    "        x, s, xtrue = x.to(device), s.to(device), xtrue.to(device)\n",
    "        mu, lpxz, lpmz, lqzx, lpz   = model(x, s, total_samples_x_train)\n",
    "        loss            = -get_notMIWAE(total_samples_x_train, lpxz, lpmz, lqzx, lpz)\n",
    "        val_loss        += loss.item()\n",
    "        batch_rmse      = compute_imputation_rmse_not_miwae(mu, lpxz, lpmz, lqzx, lpz, xtrue, s)\n",
    "        val_rmse        += batch_rmse\n",
    "val_loss /= len(val_loader)\n",
    "val_rmse /= len(val_loader)\n",
    "print(f\"Val RMSE: {val_rmse}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 1.5422841310501099\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Another type of missing data\n",
    "In this new type of missing data, we introduce missing not at random data by removing values on the first $int(n_{features}/2)$ columns of the dataset. For each of these columns, 30% of the values are removed. The removed values are the 30% closest one to the mean using the L1 norm."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:32:29.363526Z",
     "start_time": "2024-12-11T13:32:27.453212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split data into train and validation sets\n",
    "Xtrain, Xval = train_test_split(X_data, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Compute mean and standard deviation from the training set only\n",
    "mean_train  = np.mean(Xtrain.astype(np.float64), axis=0)\n",
    "std_train   = np.std(Xtrain.astype(np.float64), axis=0)\n",
    "\n",
    "# Standardize the training set using its mean and std\n",
    "Xtrain      = (Xtrain - mean_train) / std_train\n",
    "total_samples_x_train = Xtrain.shape[0]\n",
    "# Standardize the validation set using the training set's mean and std\n",
    "Xval = (Xval - mean_train) / std_train\n",
    "\n",
    "# Introduce missing data to features\n",
    "Xnan_train, Xz_train    = introduce_missing_mean_values(Xtrain)\n",
    "Xnan_val, Xz_val        = introduce_missing_mean_values(Xval)\n",
    "# Create missing data masks (1 if present, 0 if missing)\n",
    "Strain  = torch.tensor(~np.isnan(Xnan_train), dtype=torch.float32)\n",
    "Sval    = torch.tensor(~np.isnan(Xnan_val), dtype=torch.float32)\n",
    "\n",
    "Xtrain  = Xtrain.astype(np.float32)\n",
    "Xval    = Xval.astype(np.float32)\n",
    "\n",
    "# Convert features and target to PyTorch tensors\n",
    "Xnan_train  = torch.tensor(Xnan_train, dtype=torch.float32)\n",
    "Xnan_val    = torch.tensor(Xnan_val, dtype=torch.float32)\n",
    "Xtrain      = torch.tensor(Xtrain, dtype= torch.float32)\n",
    "Xval        = torch.tensor(Xval, dtype= torch.float32)\n",
    "\n",
    "# Replace missing values (NaN) with zeros for training\n",
    "Xnan_train[torch.isnan(Xnan_train)]     = 0\n",
    "Xnan_val[torch.isnan(Xnan_val)]         = 0\n",
    "\n",
    "# Prepare TensorDatasets and DataLoaders for features (Xnan_train), mask (S), and true values (Xtrain\n",
    "train_dataset   = TensorDataset(Xnan_train, Strain, Xtrain) # Features, mask, true features\n",
    "val_dataset     = TensorDataset(Xnan_val, Sval, Xval)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "# Create DataLoaders\n",
    "train_loader    = DataLoader(train_dataset, batch_size=calib_config['batch_size'], shuffle=True)\n",
    "val_loader      = DataLoader(val_dataset, batch_size=calib_config['batch_size'], shuffle=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduce missing data by removing the values around the mean\n",
      "Introduce missing data by removing the values around the mean\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:32:55.042441Z",
     "start_time": "2024-12-11T13:32:54.915348Z"
    }
   },
   "cell_type": "code",
   "source": "model = notMIWAE(n_input_features=Xtrain.shape[1], n_hidden=calib_config['n_hidden'], n_latent = calib_config['n_latent'], missing_process = calib_config['missing_process'], out_dist=calib_config['out_dist'])\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the BernouilliDecoderMiss: 870\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:33:57.931511Z",
     "start_time": "2024-12-11T13:33:07.847926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "date = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "print(f\"Training timestamp : {date}\")\n",
    "print(f\"Number of parameters in the model: {sum (p.numel() if p.requires_grad else 0 for p in model.parameters()) }\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=calib_config['lr'], weight_decay=calib_config['weight_decay'], betas=calib_config['betas'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr = calib_config['lr'],\n",
    "                                                epochs = calib_config['epochs'],\n",
    "                                                steps_per_epoch= len(train_loader),\n",
    "                                                pct_start= calib_config['pct_start'],\n",
    "                                                )\n",
    "print(f\"calib_config:{calib_config}\")\n",
    "logging.info(\"Starting training\")\n",
    "if calib_config['model'] == 'not_miwae':\n",
    "    train_notMIWAE(model, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, scheduler = scheduler, num_epochs = calib_config['epochs'], total_samples_x_train=total_samples_x_train, device = device, date=date)\n",
    "    torch.save(model.state_dict(), f\"temp/not_miwae_{date}_last_epoch.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training timestamp : 2024_12_11_14_33_07\n",
      "Number of parameters in the model: 56152\n",
      "calib_config:{'model': 'not_miwae', 'dataset_name': 'cancer', 'lr': 0.0001, 'epochs': 100, 'pct_start': 0.2, 'final_div_factor': 10000.0, 'batch_size': 16, 'n_hidden': 128, 'n_latent': 28, 'missing_process': 'linear', 'weight_decay': 0, 'betas': (0.9, 0.999), 'random_seed': 0, 'out_dist': 'gauss'}\n",
      "Epoch    1, Train Loss:  46.2630 , Train rmse:  1.6848 , Val Loss:  47.8207 , Val RMSE:  1.4676  last value of lr: 0.0000\n",
      "Epoch    2, Train Loss:  45.2744 , Train rmse:  1.7427 , Val Loss:  47.5256 , Val RMSE:  1.6143  last value of lr: 0.0000\n",
      "Epoch    3, Train Loss:  45.7471 , Train rmse:  1.6883 , Val Loss:  47.6082 , Val RMSE:  1.6952  last value of lr: 0.0000\n",
      "Epoch    4, Train Loss:  46.1216 , Train rmse:  1.6651 , Val Loss:  47.5352 , Val RMSE:  1.5348  last value of lr: 0.0000\n",
      "Epoch    5, Train Loss:  45.7331 , Train rmse:  1.6859 , Val Loss:  47.9459 , Val RMSE:  1.5873  last value of lr: 0.0000\n",
      "Epoch    6, Train Loss:  45.7893 , Train rmse:  1.7371 , Val Loss:  47.5379 , Val RMSE:  1.6349  last value of lr: 0.0000\n",
      "Epoch    7, Train Loss:  45.2331 , Train rmse:  1.7049 , Val Loss:  47.4892 , Val RMSE:  1.7376  last value of lr: 0.0000\n",
      "Epoch    8, Train Loss:  45.2913 , Train rmse:  1.6977 , Val Loss:  47.6019 , Val RMSE:  1.5170  last value of lr: 0.0000\n",
      "Epoch    9, Train Loss:  44.9585 , Train rmse:  1.6501 , Val Loss:  46.9061 , Val RMSE:  1.5777  last value of lr: 0.0000\n",
      "Epoch   10, Train Loss:  44.9479 , Train rmse:  1.6286 , Val Loss:  46.7946 , Val RMSE:  1.5542  last value of lr: 0.0001\n",
      "Epoch   11, Train Loss:  45.0438 , Train rmse:  1.6111 , Val Loss:  46.7456 , Val RMSE:  1.5294  last value of lr: 0.0001\n",
      "Epoch   12, Train Loss:  44.5735 , Train rmse:  1.6022 , Val Loss:  46.4042 , Val RMSE:  1.5209  last value of lr: 0.0001\n",
      "Epoch   13, Train Loss:  44.4465 , Train rmse:  1.5713 , Val Loss:  46.0242 , Val RMSE:  1.5294  last value of lr: 0.0001\n",
      "Epoch   14, Train Loss:  44.4256 , Train rmse:  1.5713 , Val Loss:  46.2506 , Val RMSE:  1.5006  last value of lr: 0.0001\n",
      "Epoch   15, Train Loss:  43.8630 , Train rmse:  1.5459 , Val Loss:  45.3976 , Val RMSE:  1.3905  last value of lr: 0.0001\n",
      "Epoch   16, Train Loss:  43.7815 , Train rmse:  1.5138 , Val Loss:  45.2062 , Val RMSE:  1.3134  last value of lr: 0.0001\n",
      "Epoch   17, Train Loss:  44.0100 , Train rmse:  1.4511 , Val Loss:  44.8670 , Val RMSE:  1.3623  last value of lr: 0.0001\n",
      "Epoch   18, Train Loss:  43.2184 , Train rmse:  1.5170 , Val Loss:  44.3135 , Val RMSE:  1.3191  last value of lr: 0.0001\n",
      "Epoch   19, Train Loss:  43.0550 , Train rmse:  1.4116 , Val Loss:  43.5402 , Val RMSE:  1.3060  last value of lr: 0.0001\n",
      "Epoch   20, Train Loss:  42.1518 , Train rmse:  1.4306 , Val Loss:  43.3128 , Val RMSE:  1.3919  last value of lr: 0.0001\n",
      "Epoch   21, Train Loss:  42.6428 , Train rmse:  1.3764 , Val Loss:  42.9759 , Val RMSE:  1.2470  last value of lr: 0.0001\n",
      "Epoch   22, Train Loss:  41.0937 , Train rmse:  1.4312 , Val Loss:  42.7981 , Val RMSE:  1.2645  last value of lr: 0.0001\n",
      "Epoch   23, Train Loss:  41.3038 , Train rmse:  1.4101 , Val Loss:  42.3749 , Val RMSE:  1.2940  last value of lr: 0.0001\n",
      "Epoch   24, Train Loss:  41.8484 , Train rmse:  1.3633 , Val Loss:  41.9101 , Val RMSE:  1.2668  last value of lr: 0.0001\n",
      "Epoch   25, Train Loss:  41.5014 , Train rmse:  1.3878 , Val Loss:  42.1350 , Val RMSE:  1.3002  last value of lr: 0.0001\n",
      "Epoch   26, Train Loss:  40.9399 , Train rmse:  1.3253 , Val Loss:  41.7565 , Val RMSE:  1.2543  last value of lr: 0.0001\n",
      "Epoch   27, Train Loss:  41.2129 , Train rmse:  1.3226 , Val Loss:  41.6998 , Val RMSE:  1.2572  last value of lr: 0.0001\n",
      "Epoch   28, Train Loss:  40.6068 , Train rmse:  1.3388 , Val Loss:  41.5380 , Val RMSE:  1.1328  last value of lr: 0.0001\n",
      "Epoch   29, Train Loss:  40.4450 , Train rmse:  1.3737 , Val Loss:  41.2170 , Val RMSE:  1.1904  last value of lr: 0.0001\n",
      "Epoch   30, Train Loss:  41.1817 , Train rmse:  1.3362 , Val Loss:  41.6038 , Val RMSE:  1.2123  last value of lr: 0.0001\n",
      "Epoch   31, Train Loss:  39.8929 , Train rmse:  1.3117 , Val Loss:  41.3308 , Val RMSE:  1.1485  last value of lr: 0.0001\n",
      "Epoch   32, Train Loss:  40.2714 , Train rmse:  1.3892 , Val Loss:  41.4277 , Val RMSE:  1.2678  last value of lr: 0.0001\n",
      "Epoch   33, Train Loss:  39.7621 , Train rmse:  1.3087 , Val Loss:  41.3741 , Val RMSE:  1.2898  last value of lr: 0.0001\n",
      "Epoch   34, Train Loss:  40.6121 , Train rmse:  1.3340 , Val Loss:  41.1360 , Val RMSE:  1.1762  last value of lr: 0.0001\n",
      "Epoch   35, Train Loss:  41.1363 , Train rmse:  1.2841 , Val Loss:  41.1159 , Val RMSE:  1.1041  last value of lr: 0.0001\n",
      "Epoch   36, Train Loss:  40.1892 , Train rmse:  1.2859 , Val Loss:  40.8633 , Val RMSE:  1.1014  last value of lr: 0.0001\n",
      "Epoch   37, Train Loss:  40.2074 , Train rmse:  1.2900 , Val Loss:  40.8858 , Val RMSE:  1.1736  last value of lr: 0.0001\n",
      "Epoch   38, Train Loss:  40.9039 , Train rmse:  1.2860 , Val Loss:  40.6750 , Val RMSE:  1.1403  last value of lr: 0.0001\n",
      "Epoch   39, Train Loss:  40.3655 , Train rmse:  1.2218 , Val Loss:  40.7833 , Val RMSE:  1.0830  last value of lr: 0.0001\n",
      "Epoch   40, Train Loss:  40.7328 , Train rmse:  1.2582 , Val Loss:  40.9323 , Val RMSE:  1.0939  last value of lr: 0.0001\n",
      "Epoch   41, Train Loss:  39.4948 , Train rmse:  1.2810 , Val Loss:  40.6906 , Val RMSE:  1.0831  last value of lr: 0.0001\n",
      "Epoch   42, Train Loss:  40.1223 , Train rmse:  1.2943 , Val Loss:  40.6414 , Val RMSE:  1.1730  last value of lr: 0.0001\n",
      "Epoch   43, Train Loss:  39.4900 , Train rmse:  1.2804 , Val Loss:  40.5535 , Val RMSE:  1.1353  last value of lr: 0.0001\n",
      "Epoch   44, Train Loss:  40.6359 , Train rmse:  1.2568 , Val Loss:  40.6491 , Val RMSE:  1.0488  last value of lr: 0.0001\n",
      "Epoch   45, Train Loss:  39.6554 , Train rmse:  1.2710 , Val Loss:  40.6804 , Val RMSE:  1.1113  last value of lr: 0.0001\n",
      "Epoch   46, Train Loss:  40.0906 , Train rmse:  1.2444 , Val Loss:  40.4827 , Val RMSE:  1.0906  last value of lr: 0.0001\n",
      "Epoch   47, Train Loss:  39.9917 , Train rmse:  1.2751 , Val Loss:  40.5382 , Val RMSE:  1.1988  last value of lr: 0.0001\n",
      "Epoch   48, Train Loss:  39.6455 , Train rmse:  1.2715 , Val Loss:  40.4366 , Val RMSE:  1.0358  last value of lr: 0.0001\n",
      "Epoch   49, Train Loss:  39.6405 , Train rmse:  1.2432 , Val Loss:  40.3549 , Val RMSE:  1.1038  last value of lr: 0.0001\n",
      "Epoch   50, Train Loss:  39.6732 , Train rmse:  1.2285 , Val Loss:  40.3762 , Val RMSE:  1.1539  last value of lr: 0.0001\n",
      "Epoch   51, Train Loss:  40.0936 , Train rmse:  1.3094 , Val Loss:  40.4052 , Val RMSE:  1.1573  last value of lr: 0.0001\n",
      "Epoch   52, Train Loss:  38.5804 , Train rmse:  1.2860 , Val Loss:  40.2079 , Val RMSE:  1.1111  last value of lr: 0.0001\n",
      "Epoch   53, Train Loss:  40.2748 , Train rmse:  1.2729 , Val Loss:  40.2521 , Val RMSE:  1.1493  last value of lr: 0.0001\n",
      "Epoch   54, Train Loss:  39.8276 , Train rmse:  1.2163 , Val Loss:  40.2247 , Val RMSE:  1.1492  last value of lr: 0.0001\n",
      "Epoch   55, Train Loss:  39.2312 , Train rmse:  1.2646 , Val Loss:  40.3514 , Val RMSE:  1.0816  last value of lr: 0.0001\n",
      "Epoch   56, Train Loss:  39.8472 , Train rmse:  1.2340 , Val Loss:  40.1881 , Val RMSE:  1.1262  last value of lr: 0.0001\n",
      "Epoch   57, Train Loss:  39.8935 , Train rmse:  1.2481 , Val Loss:  40.2520 , Val RMSE:  1.0659  last value of lr: 0.0001\n",
      "Epoch   58, Train Loss:  39.4517 , Train rmse:  1.2376 , Val Loss:  40.0851 , Val RMSE:  1.0368  last value of lr: 0.0001\n",
      "Epoch   59, Train Loss:  40.2619 , Train rmse:  1.2730 , Val Loss:  40.0465 , Val RMSE:  0.9987  last value of lr: 0.0001\n",
      "Epoch   60, Train Loss:  39.9483 , Train rmse:  1.2162 , Val Loss:  40.0850 , Val RMSE:  1.1223  last value of lr: 0.0000\n",
      "Epoch   61, Train Loss:  40.0677 , Train rmse:  1.2583 , Val Loss:  40.0325 , Val RMSE:  1.1543  last value of lr: 0.0000\n",
      "Epoch   62, Train Loss:  39.7277 , Train rmse:  1.2406 , Val Loss:  40.0832 , Val RMSE:  1.1431  last value of lr: 0.0000\n",
      "Epoch   63, Train Loss:  40.3012 , Train rmse:  1.2210 , Val Loss:  40.0009 , Val RMSE:  1.0495  last value of lr: 0.0000\n",
      "Epoch   64, Train Loss:  40.0442 , Train rmse:  1.2048 , Val Loss:  40.0585 , Val RMSE:  1.0951  last value of lr: 0.0000\n",
      "Epoch   65, Train Loss:  39.8186 , Train rmse:  1.2259 , Val Loss:  39.9427 , Val RMSE:  1.1406  last value of lr: 0.0000\n",
      "Epoch   66, Train Loss:  39.9684 , Train rmse:  1.2638 , Val Loss:  39.9857 , Val RMSE:  1.0829  last value of lr: 0.0000\n",
      "Epoch   67, Train Loss:  39.0982 , Train rmse:  1.2390 , Val Loss:  40.0065 , Val RMSE:  1.1179  last value of lr: 0.0000\n",
      "Epoch   68, Train Loss:  39.5618 , Train rmse:  1.2923 , Val Loss:  39.9545 , Val RMSE:  1.0480  last value of lr: 0.0000\n",
      "Epoch   69, Train Loss:  39.1949 , Train rmse:  1.2405 , Val Loss:  39.8773 , Val RMSE:  1.1353  last value of lr: 0.0000\n",
      "Epoch   70, Train Loss:  40.0774 , Train rmse:  1.2589 , Val Loss:  39.8886 , Val RMSE:  1.1081  last value of lr: 0.0000\n",
      "Epoch   71, Train Loss:  40.1670 , Train rmse:  1.2597 , Val Loss:  39.8718 , Val RMSE:  1.0587  last value of lr: 0.0000\n",
      "Epoch   72, Train Loss:  39.2636 , Train rmse:  1.2333 , Val Loss:  39.8523 , Val RMSE:  1.0674  last value of lr: 0.0000\n",
      "Epoch   73, Train Loss:  40.2003 , Train rmse:  1.2098 , Val Loss:  39.9449 , Val RMSE:  1.1091  last value of lr: 0.0000\n",
      "Epoch   74, Train Loss:  39.5203 , Train rmse:  1.2311 , Val Loss:  39.8827 , Val RMSE:  1.0888  last value of lr: 0.0000\n",
      "Epoch   75, Train Loss:  39.2783 , Train rmse:  1.2497 , Val Loss:  39.8845 , Val RMSE:  1.0408  last value of lr: 0.0000\n",
      "Epoch   76, Train Loss:  39.6140 , Train rmse:  1.2546 , Val Loss:  39.9652 , Val RMSE:  1.0332  last value of lr: 0.0000\n",
      "Epoch   77, Train Loss:  38.6626 , Train rmse:  1.2330 , Val Loss:  40.0103 , Val RMSE:  1.0431  last value of lr: 0.0000\n",
      "Epoch   78, Train Loss:  39.4351 , Train rmse:  1.2247 , Val Loss:  39.8876 , Val RMSE:  1.0531  last value of lr: 0.0000\n",
      "Epoch   79, Train Loss:  39.2225 , Train rmse:  1.2063 , Val Loss:  39.8292 , Val RMSE:  1.0115  last value of lr: 0.0000\n",
      "Epoch   80, Train Loss:  39.5146 , Train rmse:  1.1916 , Val Loss:  39.9389 , Val RMSE:  1.0878  last value of lr: 0.0000\n",
      "Epoch   81, Train Loss:  39.9550 , Train rmse:  1.2274 , Val Loss:  39.9478 , Val RMSE:  1.1246  last value of lr: 0.0000\n",
      "Epoch   82, Train Loss:  39.6883 , Train rmse:  1.2129 , Val Loss:  39.8471 , Val RMSE:  1.0809  last value of lr: 0.0000\n",
      "Epoch   83, Train Loss:  39.3417 , Train rmse:  1.1777 , Val Loss:  39.8909 , Val RMSE:  1.0265  last value of lr: 0.0000\n",
      "Epoch   84, Train Loss:  39.8090 , Train rmse:  1.1727 , Val Loss:  39.8611 , Val RMSE:  1.0864  last value of lr: 0.0000\n",
      "Epoch   85, Train Loss:  39.3018 , Train rmse:  1.2446 , Val Loss:  39.8232 , Val RMSE:  1.0672  last value of lr: 0.0000\n",
      "Epoch   86, Train Loss:  39.7955 , Train rmse:  1.2396 , Val Loss:  39.8095 , Val RMSE:  1.1062  last value of lr: 0.0000\n",
      "Epoch   87, Train Loss:  39.0486 , Train rmse:  1.1821 , Val Loss:  39.9013 , Val RMSE:  1.0476  last value of lr: 0.0000\n",
      "Epoch   88, Train Loss:  39.1143 , Train rmse:  1.2381 , Val Loss:  39.9084 , Val RMSE:  1.1177  last value of lr: 0.0000\n",
      "Epoch   89, Train Loss:  38.8418 , Train rmse:  1.1940 , Val Loss:  39.7915 , Val RMSE:  1.1396  last value of lr: 0.0000\n",
      "Epoch   90, Train Loss:  39.6436 , Train rmse:  1.2291 , Val Loss:  39.7417 , Val RMSE:  1.0855  last value of lr: 0.0000\n",
      "Epoch   91, Train Loss:  38.9051 , Train rmse:  1.2541 , Val Loss:  39.8296 , Val RMSE:  1.1283  last value of lr: 0.0000\n",
      "Epoch   92, Train Loss:  39.1734 , Train rmse:  1.2265 , Val Loss:  39.9031 , Val RMSE:  1.0245  last value of lr: 0.0000\n",
      "Epoch   93, Train Loss:  39.6002 , Train rmse:  1.2602 , Val Loss:  39.7584 , Val RMSE:  1.0271  last value of lr: 0.0000\n",
      "Epoch   94, Train Loss:  39.6083 , Train rmse:  1.2295 , Val Loss:  39.9483 , Val RMSE:  1.0561  last value of lr: 0.0000\n",
      "Epoch   95, Train Loss:  38.6581 , Train rmse:  1.2165 , Val Loss:  39.9355 , Val RMSE:  1.0405  last value of lr: 0.0000\n",
      "Epoch   96, Train Loss:  40.3103 , Train rmse:  1.2238 , Val Loss:  39.8926 , Val RMSE:  1.1063  last value of lr: 0.0000\n",
      "Epoch   97, Train Loss:  39.0129 , Train rmse:  1.2109 , Val Loss:  39.9818 , Val RMSE:  1.0802  last value of lr: 0.0000\n",
      "Epoch   98, Train Loss:  39.0545 , Train rmse:  1.2173 , Val Loss:  40.0027 , Val RMSE:  1.1010  last value of lr: 0.0000\n",
      "Epoch   99, Train Loss:  39.7589 , Train rmse:  1.1690 , Val Loss:  39.9443 , Val RMSE:  0.9899  last value of lr: 0.0000\n",
      "Epoch  100, Train Loss:  40.5343 , Train rmse:  1.2130 , Val Loss:  39.7621 , Val RMSE:  1.1620  last value of lr: 0.0000\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:34:10.341705Z",
     "start_time": "2024-12-11T13:34:10.178312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(f\"temp/not_miwae_{date}_best_val_loss.pt\", weights_only=True))\n",
    "# get best validation accuracy\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "val_rmse = 0\n",
    "with torch.no_grad():\n",
    "    for x, s, xtrue in val_loader:\n",
    "        x, s, xtrue = x.to(device), s.to(device), xtrue.to(device)\n",
    "        mu, lpxz, lpmz, lqzx, lpz   = model(x, s, total_samples_x_train)\n",
    "        loss            = -get_notMIWAE(total_samples_x_train, lpxz, lpmz, lqzx, lpz)\n",
    "        val_loss        += loss.item()\n",
    "        batch_rmse      = compute_imputation_rmse_not_miwae(mu, lpxz, lpmz, lqzx, lpz, xtrue, s)\n",
    "        val_rmse        += batch_rmse\n",
    "val_loss /= len(val_loader)\n",
    "val_rmse /= len(val_loader)\n",
    "print(f\"Val RMSE: {val_rmse}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE: 1.0372258424758911\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
